---
title: "Week 3: logistic regression"
author: "Emma Haapa"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Week 3: logistic regression {#anchor}


### A look into the data set : Student alcohol consumption

The data this week is a data set called "Student alcohol consumption" of Fabio Pagnotta and Hossain Mohammad Amran from the [UCI Machine Leaning repository](https://archive.ics.uci.edu/ml/datasets/STUDENT+ALCOHOL+CONSUMPTION). The data set povides information about students' alcohol consuption, their background information, activities and for example absences from school. The aim was to provide a means to explore if there is any correlation between the aforementioned alcohol usage and the social, gender and study time variables for each student of a math and potuguese class. There are in total 35 variables and 382 observations in the data set joined from different sets of observations based on the variables in common between the both groups in the data wrangling part of the exercise, the variables including for example "Mother's education", "Father's education", "age", "sex" and "number of school absences" from the original questionnaire. What is more, there are two new variables: "alc_use" which is combined and averaged from "Weekend alcohol consumption" and "workday alcohol consumption" and "high_use", which is essentially a variable that gets the value "TRUE", when "alc_use" is greater than 2 and "FALSE" otherwise.

The names of the variables can be seen from the R-print below, produced with the colnames()-function.


```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr) #accessed the dplyr-library
library(ggplot2) #accessed the ggplot2-library
library(readr) #accessed the readr-library
library(tidyr) #accessed tidyr-library
options(max.print=1000000)

alc <- read_csv2(file = "alc2.csv", col_names = TRUE, col_types = NULL)

colnames(alc)

```



## Relationships between the variables: hypothesis and exploration


My hypothesis is that high/low alcohol consumption is n connection to the "absences" = "number of school absences", " famrel" = "quality of family relationships",  " studytime" = "weekly study time" and "goout" = "going out with friends" variables. 

I presume that a high number of absences correlates somehow with high alcohol use, and a low number of such absences is in connection with a low usage of alcohol and as for family, that bad family relationships correlate with high alcohol consumption and respectively, good family relationships might be in connection with a lower consumption of alcohol. I also predict that a lot of time spent studying would contribute to a lower amount of consumed alcohol and going out with friends a lot would correlate with a higher usage, and in turn, not much free time and not going out much with friends would be a contributor to not using that much alcohol. 

First, I created cross tabulations of the variables in relation to the high_use-variable.


```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(gmodels)
library(gdata)
library(dplyr) #accessed the dplyr-library
library(ggplot2) #accessed the ggplot2-library
library(readr) #accessed the readr-library
library(tidyr) #accessed tidyr-library

table(high_use = alc$high_use, absences = alc$absences)
table(high_use = alc$high_use, famrel = alc$famrel)
table(high_use = alc$high_use, studytime = alc$studytime)
table(high_use = alc$high_use, goout = alc$goout)

```


I decided to make the cross tabulations with high_use instead of the whole alc_use- variable, since it seemed more clear and straghtforward to interpret the tables using that approach. It can be seen from the cross tabulations that when high_use gets the value "FALSE", i.e. alcohol consumption is less or equal to 2, there were more observations in the lower part of the absence range, i. e. there were 0-10 absences in case of one student. In case of high alcohol use, there were not as many students who had 0-10 absences, as there were those who didn't qualify for high use. In the high use case, there were also some observations in the high end of the absence scale, i.e. some students who had even 18 or 19 absences, but the diffeence to the number of observations in that end for the low alcohol use is only that there is 1 student with 18 absences present in the data.

For those students, whose family relationships were good, such as values 3-5, got a high_use value of "FALSE", and didn't qualify for high alcohol use. It could be further inspected, whether the correlation between good family relations and lower alcohol use is actually true, but looking at this table good family relations do seem to matter.

Looking at the studytime table, the students whose alcohol use was high, also used less time studying - there were more observations in the 1-2 end of the scale in case of the high_use TRUE value than there were in the 2-4 end of the scale. So those students who used higher amounts of alcohol also studied less, according to this initial observation from the cross tabulation.

Also those students, who didn't go out so much didn't seem to get high_use = TRUE -values as often as those whose going out variable got values 3-5. So going out more seems to be in connection with high alcohol usage based on the tabulation.

To further visualize these ponderings, I used ggplot, geom_bar and geom_boxplot to draw some pictures about the distributions of these variables, and in addition, a bar plot of each variable using the pipe-method introduced in the DataCamp-exercise.


```{r, echo=FALSE, message=FALSE, warning=FALSE}
g1 <- ggplot(data = alc, aes(x = alc_use, fill=sex))

# define the plot as a bar plot and draw it
g1 + geom_bar()

g2 <- ggplot(data = alc, aes(x = high_use, fill=sex))

# define the plot as a bar plot and draw it
g2 + geom_bar()

# draw a bar plot of each variable
gather(alc) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()


# initialise a plot of high_use and absences

g2 <- ggplot(alc, aes(x = high_use, y = absences, col=sex))

g3 <- g2 + ggtitle("Student absences by alcohol consumption and sex")
# define the plot as a boxplot and draw it

g3 + geom_boxplot()

# initialize a plot of high_use and G3
g4 <- ggplot(alc, aes(x = high_use, y = famrel, col=sex))

# define the plot as a boxplot and draw it
g4 + geom_boxplot() + ggtitle("Student's family relations by alcohol consumption and sex")

# initialize a plot of high_use and G3
g5 <- ggplot(alc, aes(x = high_use, y = studytime, col=sex))

# define the plot as a boxplot and draw it
g5 + geom_boxplot() + ggtitle("Student's studytime by alcohol consumption and sex")


# initialize a plot of high_use and G3
g6 <- ggplot(alc, aes(x = high_use, y = goout, col=sex))

# define the plot as a boxplot and draw it
g6 + geom_boxplot() + ggtitle("Student's going out with friends by alcohol consumption and sex")

```


The bar plots of all the variables are pretty straightforwad to interpret; they show the distribution of the values the variable takes - the frequency of a given value or an interval of values being depicted as a bar. From the bar plots of overall alcohol use and high alcohol use it can be concluded that the higher end of the scale for alc_use is less frequent, i.e. the students in this data answered more often from the 1-3 than 4-5 part of the scale. Low use seems more common among females than males, who were more frequently featured in the 4 and 5 bars of the bar plot. The high_use bar plot clarifies this observation: it clearly depicts the proportion of female in the high_use = "FALSE" value being significantly larger than that of male's, and in turn the quantity of men whose high_use = "TRUE" is larger than that of women.

In the overall bar plot picture, for example the absence variable takes values from each of the ends of the scale, more in the lower part and in the middle than in the high numbers. For studytime the value 2 is the most common, followed by 1, so the students used little time studying more often than "a lot of time", and 4 was clearly the least common value. The goout-variable's barplot seems quite normal, the most values were 3 and the ends of the scale were less frequent. As for family relationships, the value 4 was most common and right after that the value 5, which gives an impression of quite good family relations in general for the students of this data.

The box plots illustrate the 25th, 50th and 75th percentiles which are shown as "the box" in the box plot, and the lines in the fringes being the typical range. In the box plot there are also the outliers outside the lines, which are shown as discrete spots or lines.

As Tuomo Nieminen in the instruction in the Data Camp -exercise states "The whiskers" or lines that extend from the box are computed so that they extend to reach a data point that is no more than 1.5*IQR away from the box. The IQR, or the inter quartile range is defined as the 25th percentile distracted from the 75th percentile.

In first one of these box plots one can see that the typical range of absences versus high use is slightly wider and slightly upper in the scale when high use is TRUE than when it is false. Also the 25th to 75th percentile part, i.e. the box, is bigger. This goes well with the observations made from the cross tabulation before.
Also in good connection with the conclusions from the cross tabulations is the family relations box plot, where the range is upper in the scale in case of low alcohol consumption, which means that in that case there were more values that indicate good family relations in case of low consumption of alcohol.
The studytime variable's box plot shows that the range of the observations is definitely in the lower part, i. e. in the less time spent studing part of the scale, when the alcohol consumption is high, and on the other hand, the range of going out is higher on the scale when alcohol consumption is high than when it is low.

## The logistic regression model


### The summary of the model and interpretation of the coefficients


```{r, echo=FALSE, message=FALSE, warning=FALSE}
# find the model with glm()
m <- glm(high_use ~ absences + famrel + studytime + goout, data = alc, family = "binomial")

# print out a summary of the model
summary(m)

# print out the coefficients of the model
coef(m)

# compute odds ratios (OR)
OR <- coef(m) %>% exp

# compute confidence intervals (CI)
CI <- confint(m) %>% exp

# print out the odds ratios with their confidence intervals
cbind(OR, CI)



```



The summary of the fitted model with high_use as the target variable and absences, famrel, studytime and goout as the explanatory variables. If the odds ratio equals to 1, there is no statistical connection between the target variable and the explanatory variable in question. However,
hypothesis tests for the odds ratio can not used to determine statistical significance of the association between the variables in the model, like last week. Instead, confidence intervals of OR are constructed and used to determine whether or not the association is statistically significant. The confidence interval for an odds ratio has the same general formula as any confidence interval:
Point Estimate + Confidence Coefficient multiplied by Standard Error, 
and Point Estimate - Confidence Coefficient multiplied by Standard Error. The difference is that the confidence interval of the odds ratio is calculated on a natural logarithm scale and then converted back to the original scale, whereas this is not needed in the case of a population mean, for example. The statistical signigficance is then determined as follows: if the 95% Confidence Interval does not contain the value 1.0, the association is statistically significant at alpha = 0,05.

The exponents of the printed coefficients can be interpreted as the odd ratios between a unit change in the corresponding explanatory variable, versus no change in that explanatory variable. To illustrate this with an example: if there is a unit change in absences the student is 1,07 times more likely to be inclined towards high-alcohol usage, which tends to say that high number of absences tells something about alcohol usage and the two variables are possibly in some sort of statistical relationship, also reassuring that my initial hypothesis regarding the higer number of absences and high use of alcohol being in positive connection. 

In turn, if there's a unit change in family relations, the person is only 0,71 times more likely to incline towards high usage, so if there is no change in famrel, the person is 1/0,71=1,41 times more likely to high use. Gathering from that the hypothesis on bad family relations being connected to higher use of alcohol seems to be legit, as no change in the explanatory variable gives the result of the student being more likely to use more alcohol. In this case there's also a possibility of a statistically significant relationship between the explanatory and the target variable.

As for how the students spend their spare time, for time spent studying the exponent of the coefficient i.e. the odds ratio is only 0.57 which means that if there's no change in the studytime variable the student is even 1/0,57 = 1,75 times more likely to high use. It seems that my hypothesis with regards to this variable and its connection with high use is also more in the right than in the wrong. Consequently, going out with friends and high use seem to have the strongest connection: a unit change in the variable goout means that the student is 2,14 times more likely to use a lot of alcohol, which is the largest number in the exponents of the coefficients table. It would mean that the hypothesis is correct in light of this initial exploration and this paticular model, and also goes well together with the intuition that the more time spend hanging out with a group of friends the more likely the student is to use alcohol.

From the above coefficient/confidence interval table it can also be seen that none of the confidence intervals contain the value 1,0, which indicates that the above statements considering the explanatory variables are somewhat true and their relationship to the target variable are statistically significant at confidence level 0,05.


```{r}
# fit the model
m <- glm(high_use ~ absences + famrel + studytime + goout, data = alc, family = "binomial")

# predict() the probability of high_use
probabilities <- predict(m, type = "response")

# add the predicted probabilities to 'alc'
alc <- mutate(alc, probability = probabilities)

# use the probabilities to make a prediction of high_use
alc <- mutate(alc, prediction = (probability > 0.5))

# tabulate the target variable versus the predictions
table(high_use = alc$high_use, prediction = alc$prediction)

```


The predict() function can be used to make predictions with the model.  By using predict() on the model object and mutating the alc data again by adding a column 'probability' with the predicted probabilities, adding a column 'prediction' which is true if the value of 'probability' is greater than 0.5, I produced a confusion matrix or the cross table above. It shows how good the model is in terms of predicting: it can be seen that when high_use is false, 242 times the prediction with this model is also false and only 26 times actually true (when the prediction would be wrong), and when high_use is true, the prediction is false in 65 cases and true in 49 cases. This is a little bit confusing, and gives the impression that predictions in case of the actual value being true are not accurate based on this model, but when the actual calue is false the predictions tend to be more accurate than not. 



```{r}
# access dplyr and ggplot2
library(dplyr); library(ggplot2)

# initialize a plot of 'high_use' versus 'probability' in 'alc'
g <- ggplot(alc, aes(x = probability, y = high_use, col = prediction))

# define the geom as points and draw the plot
g + geom_point()

# tabulate the target variable versus the predictions
table(high_use = alc$high_use, prediction = alc$prediction) %>% prop.table %>% addmargins
```



By drawing the plot of actual values vs. predictions and the probability cross table of actual values vs. predictions, the proportions of the false and true predictions are more visually comprehensible.
In the case of binary classification the wellness of the model can be determined by looking at its accuracy, which means the average number of observations that are correctly classified. Logistic regression is a classification method, in which the aim is to minimize the number of wrongly classified observations. The notion of penalty or loss function for the logistic regression describes the mean of the wrongly classified observations or in this case, students. The less penalty, the better the model.

First step is to define the loss function as the average of the wrong predictions.


```{r}
# define a loss function (mean prediction error)
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}

loss_func(class = alc$high_use, prob = alc$probability)


```



This model has an average of 0,238 wrong predictions or inaccurately classified individuals, which is also called the training error. The value is quite good, considering the model setting, since it is significantly less than 0,5. Surely it could be less for the model to be extremely good - on average, 1 in 4 predictions is wrong - but at this stage, it is not an alarming number, since roughly over 75 percent of the predictions are correct. This model is much better than, for example, tossing a coin to predict the values of the variables.


```{r, echo=FALSE, warning=FALSE}
# define a loss function (average prediction error)
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}

# compute the average number of wrong predictions in the (training) data
loss_func(class = alc$high_use, prob = alc$probability)

nrow(alc)

# K-fold cross-validation
library(boot)
cv <- cv.glm(data = alc, cost = loss_func, glmfit = m, K = 10)

# average number of wrong predictions in the cross validation
cv$delta[1]
```


It can be seen from the above that my model has a better test set performance (only 0,2382199 prediction error using the 10-fold cross validation) than the model used in the DataCamp exercise, which had an error of 0,26.   


