---
title: 'Course: Introduction to open data science'
author: "Emma Haapa"
---
c

# Week 2: Regression and model validation {#anchor}

The [data](http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt) we wrangled and analyzed this week is called JYTOPKYS3 by Kimmo Vehkalahti, which I named "learning14" for the course of the initial wrangling assignments. The number of observations in the data is 183 and there are in total 66 variables.

The variables, essentially the statements or questions in the questionnaire, are coded in short sections of letters and numbers, for example "SU08" = "I tend to read very little beyond what is actually required to pass", "D06" = "I look at the evidence carefully and try to reach my own conclusion about what I'm studying" and "ST01" = "I manage to find conditions for studying which allow me to get on with my work easily", the initials of which in turn correspond to their function in measuring the student's deep, surface or strategic inclination towards learning. In other words, the combinations of variables are meters which measure the dimensions of deep, strategic and surface learning. 

There are also background variables in the data, such as "gender" and "age", and variables that measure more general things associated with learning, succeeding in the learning process and how much time the student spends studying. The study was conducted in 2014 as an international study project with funding from Opettajien Akatemia (see this [link](http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-meta.txt) for further clarification).

For the assignment this week, the data was wrangled, so that the dimensions of the data were taken into consideration when the data was formed into columns (deep, surf, stra) from all the variables associated with them and averaging by the number of questions. Also the variables 'gender', 'points' and 'attitude' were taken into the formed data set "learning2014". Also, in the data wrangling part of the exercise, the observations in which the variable "points" got the value 0, were excluded from the data, leaving only 166 observations out of the initial 183 in the final data. The dimensions of the data set were then accordingly:

The structure of the data can be further explored by using the str()- and head()-functions in R.

```{r First chunk, include=FALSE}

learning14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)

library(dplyr) #accessed tge dplyr-library

learning14$Attitude
learning14$Attitude / 10
learning14$attitude <- learning14$Attitude / 10


# Creating objects that refer to all the questions that assess different types of learning: deep, surface and strategic learning.
deep_questions <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D06",  "D15", "D23", "D31")
surface_questions <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
strategic_questions <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")

# Selecting columns that have to do with deep learning and creating a column deep by averaging.
deep_columns <- select(learning14, one_of(deep_questions))
learning14$deep <- rowMeans(deep_columns)

# Selecting columns that have to do with surface learning and creating a column surf by averaging.
surface_columns <- select(learning14, one_of(surface_questions))
learning14$surf <- rowMeans(surface_columns)

# Selecting columns that have to do with strategic learning and creating a column stra by averaging.
strategic_columns <- select(learning14, one_of(strategic_questions))
learning14$stra <- rowMeans(strategic_columns)

keep_columns <- c("gender","Age","attitude", "deep", "stra", "surf", "Points")

# Creating a new data set
learning2014 <- select(learning14, one_of(keep_columns))

# Changing the names of the columns

colnames(learning2014)[2] <- "age"
colnames(learning2014)[7] <- "points"

# Selecting the rows where points is greater than zero
learning2014 <- filter(learning2014, points > 0)
```

```{r chunk2, echo=TRUE}
str(learning2014)
head(learning2014)

```

of which the first shows that the variables 'age' and 'points' only get integer values, whereas 'attitude', 'deep', 'stra' and 'surf' get numerical values. The gender variable is a 2-level factor, with the values 1 or 2, 1 being "F" or 'female' and 2 being "M" or 'male'. The latter, in turn, shows the first parts of the vectors in the data, in this case, the first 6 observations, in detail (all of the variables' values of this observation).

### The relationships between the variables 

First, as an example, I will follow the idea in the DataCamp exercise and draw a plot of attitude versus exam points with gender specifying the plots. Also a regression line is added to the plots, one for both genders (if gender is male, the line is blue and if female, the line is red.) The regression line is fitted using the least squares approach.

```{r Codecunk3, include=FALSE}
options(max.print=1000000)
learning2014 <- read.table(file = "learning2014.txt")

library(dplyr) #accessed the dplyr-library
library(ggplot2) #accessed the ggplot2-library
```

```{r Codechunk4, echo=FALSE}
p1 <- ggplot(learning2014, aes(x = attitude, y = points, col =gender))

# define the visualization type (points)
p2 <- p1 + geom_point()

# add a regression line
p3 <- p2 + geom_smooth(method = "lm")

# add a main title and draw the plot
p4 <- p3 + ggtitle("Student's attitude versus exam points")
p4
```


I attempted to make a similar scatter plot out of two other variables in the learning2014 data, for learning's sake. I decided to choose variables "deep" and "points", to see if the student's deep leaning skills are in some kind of connection or loosely said, correlation, with each other.

```{r Codechunk5, echo=FALSE}

p11 <- ggplot(learning2014, aes(x = deep, y = points, col =gender))

# define the visualization type (points)
p22 <- p1 + geom_point()

# add a regression line
p33 <- p2 + geom_smooth(method = "lm")

# add a main title and draw the plot
p44 <- p3 + ggtitle("Student's deep learning skills versus exam points")
p44

```


As can be seen from the scatter plot and the linear regression curve fitted into it, there seems to be less connection between a student's deep learning skills and their points earned in an exam, as the regression line is practically vertical.
From this I got the idea to also illustrate the connection between a student'sstrategic leaning skills with thei exam points, as I thought their strategic learning skills would have more to do with their points in an exam. Though the following code I got the resulting picture:


```{r Codechunk6, echo=FALSE}

p111 <- ggplot(learning2014, aes(x = stra, y = points, col =gender))

# define the visualization type (points)
p222 <- p1 + geom_point()

# add a regression line
p333 <- p2 + geom_smooth(method = "lm")

# add a main title and draw the plot
p444 <- p3 + ggtitle("Student's strategic learning skills versus exam points")
p444
ggsave("plot2.png", width = 5, height = 5)

```


From this I concluded, that there seems to be more connection with a students strategic learning skills and their exam points, at least more so than with their deep leaning skills. The regression line in this case, is an ascending line i. e. the slope of the line is positive. This tends to say that the more the student is inclined towads strategic learning, the bigger number of points they seem to score from an exam. However, these are just initial observations, and have little if nothing to do with actual statistical analysis - but based on these ponderings and inclinations I would, as a statistician, try to establish my further work on this notion.

The following illustrates the simple regression between a few of the variables:

```{r, echo=FALSE}
library(ggplot2)
qplot(attitude, points, data = learning2014) + geom_smooth(method = "lm")

# fit a linear model
my_model <- lm(points ~ attitude, data = learning2014)

# print out a summary of the model
summary(my_model)
```

```{r, echo=FALSE}
library(ggplot2)
qplot(deep, points, data = learning2014) + geom_smooth(method = "lm")

# Fit a linear model
my_model1 <- lm(points ~ deep, data = learning2014)

# Summary of the model
summary(my_model1)
```

```{r, echo=FALSE}
library(ggplot2)
qplot(stra, points, data = learning2014) + geom_smooth(method = "lm")

# Fit a linear model
my_model2 <- lm(points ~ stra, data = learning2014)

# Print out a summary of the model
summary(my_model2)
```

```{r, echo=FALSE}
library(ggplot2)
qplot(deep, attitude, data = learning2014) + geom_smooth(method = "lm")

# Fit a linear model
my_model3 <- lm(attitude ~ deep, data = learning2014)

# Print out a summary of the model
summary(my_model3)
```


The statistical test related to the parameters is a test for the null-hypothesis, that the beta-parameter in the regression model is actually zero. The p-value indicates the probability of that happening, using this statistical model. The influence of an explanatory variable is statistically significant, if it (in general) is <0,05. The highest value the p-value could take to be statistically at least somewhat significant, is 0,1.

In the above summary-tables, the p-values are for attitude 4.12e-09, which is extremely good, and strictly less than 0,001 which is the best of the signifigance levels. So the influence of attitude to points seems to be significant, and reading from the table, the first estimate, i.e. the beta-parameter, is 3,5255 which indicates a strong positive influence on points. So a good attitude correlates with good points, simply said. The p-value for deep is 0.897, which is not at all a good p-value - it is significantly greater than 0,1, so the influence of deep on points cannot be considered statistically significant. For stra, the p-value is 0.0603, which is pretty good - it is close to 0,05 and definitely less than 0,1.

Attempting to find variables that have statistically significant relationships with the points-variable, I tried many options, one of which was the combination of the variables 'deep', 'stra', and 'surf' as the explanatory variables, which gave puzzling results. 

```{r, echo=FALSE}
library(ggplot2)
library(GGally)
# create an plot matrix with ggpairs()
ggpairs(learning2014, lower = list(combo = wrap("facethist", bins = 20)))

# create a regression model with multiple explanatory variables
my_model4 <- lm(points ~ deep + stra + surf, data = learning2014)

# print out a summary of the model
summary(my_model4)
```

Fitting the model, the beta-parameters could be read from the summary-table: for the variable deep the beta-parameter is -0,7443, stra 0,9878 and surf -1,6296. The p value for each of these is, however, greater than 0,05, and for deep the biggest of all. As 0,05 is considered to be the general, known limit for statistical signifigance, the variable deep cannot be considered as an influencing variable for exam points. 

To reassign my model, I set the variables 'attitude', 'stra' and 'surf' as the explanatory variables.

```{r}
library(ggplot2)
library(GGally)
# create an plot matrix with ggpairs()
ggpairs(learning2014, lower = list(combo = wrap("facethist", bins = 20)))

# create a regression model with multiple explanatory variables
my_model4 <- lm(points ~ attitude + stra + surf, data = learning2014)

# print out a summary of the model
summary(my_model4)
```

The summary-table shows the beta-parameters for each of the aforementioned variables, and also that the p-value for 'attitude' is 0,0000000193 - which is by all means strictly less than 0,05! Now, with this model, at least for the attitude parameter, the probability of making a Type I -error (discarding error) is very little. For the other parameters, stra and surf, both have >0,1 p-values, which gives the results signifigance level to be quite low and the probability of making the Type I -error (of discaring the null hypothesis) slightly greater.

Fitting the model without the slightly problematic 'surf'-variable, helps a little, as can be seen from the summary table below: 

```{r, echo=FALSE}
library(ggplot2)
library(GGally)
# create an plot matrix with ggpairs()
ggpairs(learning2014, lower = list(combo = wrap("facethist", bins = 20)))

# create a regression model with multiple explanatory variables
my_model4 <- lm(points ~ attitude + stra, data = learning2014)

# print out a summary of the model
summary(my_model4)

```

Now the p-value of attitude is even less (0,00000000631) than with the third variable present in the model, so the statistical signifigance of this variable's influence to the main variable is strictly <0,001. In this model, also 'stra''s statistical signifigance is greater than before, it is 0,08927, which in turn is greater than 0,1 and makes the variable's influence on the main variable statistically significant. What is more, it is, according to my attempts, the best result out of all the combinations I tried. 

### Interpreting the model parameters

The model parameters can be read from the summary-table. The alpha-parameter indicates, where the regression line intercepts the y-axis. In this case it is 8,9729. The beta-parameter for attitude in this model is 3,4658, which indicates a strong positive influence of attitude on exam points, i. e. the explanatory and target variable. The beta-parameter can be viewed as the slope of the curve which indicates the influence of this variable on the target variable; in this case the line is ascending.  In the case of the stra-variable, the beta-parameter is a little bit smaller in value, but still positive, so it can be said that the strategic learning style has a positive influence on exam points - only a bit lesser of an influence than the attitude-variable has.

R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as 'the coefficient of determination', or 'the coefficient of multiple determination' for multiple regression, as in this case.
By definition, R-squared is the percentage of the response variable variation that is explained by a linear model. In other words: how well the model explains the main variables variation, as opposed to other factors that might affect the main variable. It can be written as

R-squared = Explained variation / total variation

R-squared is always between 0 and 100%:
If R-squared is 0% that indicates the model explains none of the variability of the response data around its mean.
If R-squared is 100% that indicates that the model explains all the variability of the response data around its mean.

In general, the higher the R-squared, the better. 
Taking into consideration this notion, the multiple R-squared is not very high in either of the latter cases: for the three variable-regression it is 0,2048 or 20,48% and for the two-variable-regression slightly bigger, 0.2074 or 20,74%. Concluding from that, the model fits a little better when there are only two explanatory variables, 'attitude' and 'stra', than if 'surf' were added to the model.
Using a summary of your fitted model, explain the relationship between the chosen explanatory variables and the target variable (interpret the model parameters). Explain and interpret the multiple R squared of the model.

### Graphical model validation

```{r, echo=TRUE}
# create a regression model with multiple explanatory variables
my_model4 <- lm(points ~ attitude + stra, data = learning2014)

# draw diagnostic plots using the plot() function. Choose the plots 1, 2 and 5
par(mfrow = c(2,2))
plot(my_model4, which=c(2,5,1))
```



In linear regression the assumptions include that the target variable is assumed to be a linear combination of the explanatory variables - i. e. the fisrt big assumption associated with a linear model is linearity. Generally, it is useful to assume also, that the errors are normally distributed and moreover, with a mean of 0 and a constant variance. This includes also the notion that the errors are not correlated and that the size of an error does not depend on the explanatory variables. There are a number of graphical ways to explore these assumptions, for example:


### The Residuals vs. Fitted values -plot

The Residuals vs. Fitted values -plot explores the constant variance assumption of the errors in the model, in other words the assumption that there should be no or close to none dependency between the errors and the explanatory variables. The simple plot shows residuals and model predictions, and if there seems to be any pattern in the plot, one could conclude that there is some dependency and the variance of the errors is not constant. 

In the plot of the model, the points seem to be distributed quite randomly, and no actual pattern can really be seen. The curve fitted to these points is also rather vertical, with no deviation or slope differing from 1, so it is a good sign that there is no dependency of the errors on the explanatory variables. In this case, I would say that the variance seems to be constant, and this assumption of the linear model holds true.

### The Normal QQ-plot

The Normal QQ-plot is a plot of the residuals that can be used to explore the assumption of the normality of the errors in the model. In the Normal QQ-plot of the model in question, the points of the plot seem to fall pretty nicely on the curve, and nothing seems to deviate out of the line. Consequently, according to this graphical model validation plot, the model's errors are (at least close to being) normally distributed and the normality assumption is correct.

### The Residual vs. Leverage -plot

The Residueal vs. Leverage -plot measures, how much impact a signle observation has on the model. By analyzing the plot, any observations that have a high impact on the model, can be identified. The model of the two explanatory variables affecting the points-variable, we can see that the scale of the Leverage-plot is not very wide - only from 0 to 0,05. This indicates, that no observation is stranded outside the plot, and if there are observations in the fringes of the plot, the distance to other observations is not so big. Also, all the observations seem to fall in the plot quite evenly, which in itself indicates that there are no single observations having unusually high impact on the model. So the linearity assumption can be said to be correct in this model.

